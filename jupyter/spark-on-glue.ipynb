{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Spark on AWS Glue\n",
    "\n",
    "The following notebook was created in Sagemaker on a Glue dev endpoint. This contains examples of how to run pure PySpark on AWS Glue, avoiding Amazon's Glue subclass/library.\n",
    "\n",
    "Note that AWS Sagemaker is literally exactly AWS-managed Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>41</td><td>application_1597098563813_0042</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-28-38-12.ec2.internal:20888/proxy/application_1597098563813_0042/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-28-41-193.ec2.internal:8042/node/containerlogs/container_1597098563813_0042_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from awsglue.context import GlueContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "import dateutil.tz\n",
    "\n",
    "load_datetime = datetime.now()  # I typically append this to tables for later ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### env\n",
    "\n",
    "I keep a file called env.py in S3 that contains info concerning the AWS environment needed, this cell replicates that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prod = {\"s3tmp\": \"s3://company-redshift-prod/glue-temp/\",\n",
    "        \"s3src\": \"s3://company-redshift-prod/data-sources/\",\n",
    "        \"rsconn\": \"company-redshift\"\n",
    "        }\n",
    "\n",
    "env = prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create contexts\n",
    "\n",
    "Sagemaker requires a little different syntax for the Spark context than a Glue script as noted here. You will need to comment/uncomment as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create necessary contexts\n",
    "sc = spark.sparkContext  # Jupyter (use this in Jupyter/Sagemaker)\n",
    "# sc = SparkContext()  # Glue (use this in actual Glue scripts)\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get info from Glue connection(s)\n",
    "\n",
    "This Glue method essentially treats a Glue connection as a credential store and allows you to pull in creds for use in connection URLs without exposing secrets. This is the only time I use the actual GlueContext(), everything else is pure Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get creds from redshift connection\n",
    "conf = glueContext.extract_jdbc_conf(env[\"rsconn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark read on S3\n",
    "\n",
    "Note here we can now use a pure spark read on S3. Here we read a csv of public traffic data downloaded from https://catalog.data.gov/dataset?res_format=CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.load(env[\"s3src\"] + \"monroe-county-crash-data2003-to-2015.csv\",\n",
    "                    format=\"com.databricks.spark.csv\",\n",
    "                    header=\"true\",\n",
    "                    inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Master Record Number=902363382, Year=2015, Month=1, Day=5, Weekend?='Weekday', Hour=0, Collision Type='2-Car', Injury Type='No injury/unknown', Primary Factor='OTHER (DRIVER) - EXPLAIN IN NARRATIVE', Reported_Location='1ST & FESS', Latitude=39.15920668, Longitude=-86.52587356)"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Master Record Number', 'int'), ('Year', 'int'), ('Month', 'int'), ('Day', 'int'), ('Weekend?', 'string'), ('Hour', 'int'), ('Collision Type', 'string'), ('Injury Type', 'string'), ('Primary Factor', 'string'), ('Reported_Location', 'string'), ('Latitude', 'double'), ('Longitude', 'double')]"
     ]
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+---+--------+----+--------------+------------------+--------------------+--------------------+-----------+------------+\n",
      "|Master Record Number|Year|Month|Day|Weekend?|Hour|Collision Type|       Injury Type|      Primary Factor|   Reported_Location|   Latitude|   Longitude|\n",
      "+--------------------+----+-----+---+--------+----+--------------+------------------+--------------------+--------------------+-----------+------------+\n",
      "|           902363382|2015|    1|  5| Weekday|   0|         2-Car| No injury/unknown|OTHER (DRIVER) - ...|          1ST & FESS|39.15920668|-86.52587356|\n",
      "|           902364268|2015|    1|  6| Weekday|1500|         2-Car| No injury/unknown|FOLLOWING TOO CLO...|       2ND & COLLEGE|   39.16144|  -86.534848|\n",
      "|           902364412|2015|    1|  6| Weekend|2300|         2-Car|Non-incapacitating|DISREGARD SIGNAL/...|BASSWOOD & BLOOMF...|39.14978027|-86.56889006|\n",
      "|           902364551|2015|    1|  7| Weekend| 900|         2-Car|Non-incapacitating|FAILURE TO YIELD ...|      GATES & JACOBS|  39.165655|-86.57595635|\n",
      "|           902364615|2015|    1|  7| Weekend|1100|         2-Car| No injury/unknown|FAILURE TO YIELD ...|               W 3RD|  39.164848|-86.57962482|\n",
      "+--------------------+----+-----+---+--------+----+--------------+------------------+--------------------+--------------------+-----------+------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark select\n",
    "\n",
    "We can select columns like any Spark dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+--------------+------------------+--------------------+\n",
      "|year|month|day|hour|collision_type|       injury_type|  DWH_LOAD_TIMESTAMP|\n",
      "+----+-----+---+----+--------------+------------------+--------------------+\n",
      "|2015|    1|  5|   0|         2-Car| No injury/unknown|2020-10-12 02:54:...|\n",
      "|2015|    1|  6|1500|         2-Car| No injury/unknown|2020-10-12 02:54:...|\n",
      "|2015|    1|  6|2300|         2-Car|Non-incapacitating|2020-10-12 02:54:...|\n",
      "|2015|    1|  7| 900|         2-Car|Non-incapacitating|2020-10-12 02:54:...|\n",
      "|2015|    1|  7|1100|         2-Car| No injury/unknown|2020-10-12 02:54:...|\n",
      "+----+-----+---+----+--------------+------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_select = df.select(\n",
    "    F.col(\"year\"),\n",
    "    F.col(\"month\"),\n",
    "    F.col(\"day\"),\n",
    "    F.col(\"hour\"),\n",
    "    F.col(\"Collision Type\").alias('collision_type'),\n",
    "    F.col(\"Injury Type\").alias('injury_type'),\n",
    "    F.lit(load_datetime).alias(\"DWH_LOAD_TIMESTAMP\"))\n",
    "\n",
    "df_select.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "\n",
    "We can do the same with spark.sql() as shown below. Note that in Sagemaker/Jupyter the temp view must be created in the same cell as the spark.sql() that references it or it will not work.\n",
    "\n",
    "Note also we convert our date/time fields to a true timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+------------------+--------------------+\n",
      "|           datetime|collision_type|       injury_type|  DWH_LOAD_TIMESTAMP|\n",
      "+-------------------+--------------+------------------+--------------------+\n",
      "|2015-01-05 00:00:00|         2-Car| No injury/unknown|2020-10-12 02:54:...|\n",
      "|2015-01-06 15:00:00|         2-Car| No injury/unknown|2020-10-12 02:54:...|\n",
      "|2015-01-06 23:00:00|         2-Car|Non-incapacitating|2020-10-12 02:54:...|\n",
      "|2015-01-07 09:00:00|         2-Car|Non-incapacitating|2020-10-12 02:54:...|\n",
      "|2015-01-07 11:00:00|         2-Car| No injury/unknown|2020-10-12 02:54:...|\n",
      "+-------------------+--------------+------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"crash_data\")\n",
    "df_sql = spark.sql(\"\"\"select\n",
    "                    to_timestamp(concat(cast(year as string), '-', \n",
    "                        cast(month as string), '-', cast(day as string), ' ', \n",
    "                            lpad(cast(hour as string),4,'0')), \"yyyy-MM-dd HHmm\") as datetime\n",
    "                    ,`Collision Type` as collision_type\n",
    "                    ,`Injury Type` as injury_type\n",
    "                    from crash_data\"\"\") \\\n",
    "        .withColumn(\"DWH_LOAD_TIMESTAMP\", F.lit(load_datetime))\n",
    "\n",
    "df_sql.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark read on a jdbc connection\n",
    "\n",
    "It is also extremely possible to do a Spark read from something like Amazon RDS or Redshift. Here are dummy examples showing syntax for both using a direct query, note that .option(\"dbtable\", table) would be used for a full table read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for RDS connection, note the weird syntax required to use a query\n",
    "\n",
    "conf1 = glueContext.extract_jdbc_conf(\"name_of_rds_connection\")\n",
    "\n",
    "qy = \"select column1, column2, column3 from rds_table\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", conf1[\"url\"] + \"/database_name\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\",\n",
    "            f\"({qy}) a\") \\\n",
    "    .option(\"user\", conf1[\"user\"]) \\\n",
    "    .option(\"password\", conf1[\"password\"]) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for Redshift, note there is a \"query\" option\n",
    "\n",
    "url = f\"{conf['url']}/dwh_test?user={conf['user']}&password={conf['password']}\"\n",
    "\n",
    "qy = \"select column1, column2, column3 from redshift_table\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"com.databricks.spark.redshift\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"query\", qy) \\\n",
    "    .option(\"forward_spark_s3_credentials\", 'true') \\\n",
    "    .option(\"tempdir\", env[\"s3tmp\"]) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write to Redshift\n",
    "\n",
    "Now we can Spark write to Redshift (you know you want to). Note here we reference our connection creds from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = f\"{conf['url']}/dwh_test?user={conf['user']}&password={conf['password']}\"\n",
    "table = 'staging.crash_data'\n",
    "\n",
    "df_sql.write \\\n",
    "    .format(\"com.databricks.spark.redshift\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"tempdir\", env[\"s3tmp\"]) \\\n",
    "    .option(\"forward_spark_s3_credentials\", \"true\") \\\n",
    "    .mode(\"append\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postactions and preactions\n",
    "\n",
    "This example does not contain an instance of a preaction or postaction, but these are invaluable options on Spark write and worth looking into. These allow you to run SQL before and/or after a write. Below is a dummy version to show the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"my_table\"\n",
    "\n",
    "# using delete instead of truncate will empty a table as a TRANSACTION and will rollback on Spark write fail (!)\n",
    "preactions = \"delete from my_table where 1=1;\" \n",
    "\n",
    "# you can call an sp or do other cleanup afterwards\n",
    "postactions = \"call public.some_stored_procedure();\"\n",
    "\n",
    "df.write \\\n",
    "    .format(\"com.databricks.spark.redshift\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"tempdir\", env[\"s3tmp\"]) \\\n",
    "    .option(\"preactions\", preactions) \\\n",
    "    .option(\"postactions\", postactions) \\\n",
    "    .option(\"forward_spark_s3_credentials\", \"true\") \\\n",
    "    .mode(\"append\") \\\n",
    ".save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
